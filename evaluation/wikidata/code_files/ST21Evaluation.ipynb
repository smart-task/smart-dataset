{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for evaluation\n",
    "class fc_eval:\n",
    "     def __init__(self):\n",
    "        #count for dumy questions having predicted answers\n",
    "        self.cntr_dummy_ques = 0\n",
    "\n",
    "     #function for evaluation if category is boolean\n",
    "     def fetch_rank_bool(self, catg_pred, catg_correct):\n",
    "         if catg_pred == catg_correct:\n",
    "            rank = 1\n",
    "         else:\n",
    "            rank = 0\n",
    "         return rank\n",
    "\n",
    "     #function for evaluation if category is literal or resource\n",
    "     def fetch_rank_literal_resource(self, catg_pred, catg_correct, lst_pred, lst_correct):\n",
    "         if catg_pred == catg_correct:\n",
    "            check = any(item in lst_pred for item in lst_correct)\n",
    "            if check:\n",
    "               for j in range(len(lst_pred)): \n",
    "                   for i in range(len(lst_correct)):\n",
    "                       if lst_pred[j] == lst_correct[i]:        \n",
    "                          #print(lst_pred[j] + ' is present in correct list')\n",
    "                          rank = lst_correct.index(lst_pred[j]) + 1\n",
    "                          break\n",
    "            else:\n",
    "               rank = 0            \n",
    "         else:\n",
    "            rank = 0\n",
    "         return rank\n",
    "     \n",
    "\n",
    "     #function to evaluate answer types\n",
    "     def evaluate(self, inp_file_correct, inp_file_pred, out_file_path, results_file):\n",
    "        json_collection = []\n",
    "        #opening dummy predictions file\n",
    "        with codecs.open(inp_file_pred, 'r', 'utf-8-sig') as data_file: \n",
    "            json_data = json.load(data_file)\n",
    "            pred_data_df = pd.DataFrame(json_data)\n",
    "        #opening correct lcquad2 ans type file\n",
    "        with codecs.open(inp_file_correct, 'r', 'utf-8-sig') as df_correct: \n",
    "             j_data_correct = json.load(df_correct)\n",
    "             correct_data_df = pd.DataFrame(j_data_correct)\n",
    "        cntr_ques_matched = 0\n",
    "        catg_matched = 0\n",
    "        rank = 0\n",
    "        rr = 0\n",
    "        sum_rr = 0\n",
    "        mrr = 0\n",
    "        #iterating through the dummy predictions file\n",
    "        for i in range(len(pred_data_df['id'])):\n",
    "            self.cntr_dummy_ques += 1\n",
    "            #creating a reduced data frame out of entire correct data file, which had data of only 1 concerned question in each iteration\n",
    "            sel_df = correct_data_df.loc[correct_data_df['id']==pred_data_df['id'][i]]\n",
    "            if not(sel_df.empty):\n",
    "               #checking if questions are same in dummy prediction file and correct file\n",
    "               if (pred_data_df['id'][i] == list(sel_df['id'])[0]):\n",
    "                  cntr_ques_matched += 1\n",
    "                  #print('question matched')\n",
    "                  #if category does not match, then assigning rank = 0\n",
    "                  if (pred_data_df['category'][i] != list(sel_df['category'])[0]):\n",
    "                     rank = 0\n",
    "                  #if category matches\n",
    "                  else:\n",
    "                     catg_matched += 1\n",
    "                     #print('category matched')\n",
    "                     if list(sel_df['category'])[0] == 'boolean':\n",
    "                        rank = self.fetch_rank_bool(pred_data_df['category'][i], list(sel_df['category'])[0])\n",
    "                     elif (list(sel_df['category'])[0] == 'literal' or list(sel_df['category'])[0] == 'resource'):\n",
    "                        #print('not boolean')\n",
    "                        rank = self.fetch_rank_literal_resource(pred_data_df['category'][i], list(sel_df['category'])[0], list(pred_data_df['type'][i]), list(sel_df['type'])[0])\n",
    "                    \n",
    "            #in case correct ans typ file has no question matching the file with dummy question predictions\n",
    "            else:\n",
    "              rank = 0                \n",
    "            #assigning values to reciprocal rank i.e. to rr\n",
    "            if rank!= 0:\n",
    "               rr = 1/rank\n",
    "            else:\n",
    "               rr = 0\n",
    "\n",
    "            #framing output file format\n",
    "            try:      \n",
    "                     data = {}\n",
    "                     try:\n",
    "                         data['id'] = int(pred_data_df['id'][i])\n",
    "                     except:\n",
    "                         data['id'] = ''\n",
    "                         pass\n",
    "                     try:\n",
    "                         data['question'] = pred_data_df['question'][i]\n",
    "                     except:\n",
    "                         data['question'] = ''\n",
    "                         pass\n",
    "                     try:\n",
    "                         data['category'] = pred_data_df['category'][i]\n",
    "                     except:\n",
    "                         data['category'] = ''\n",
    "                         pass\n",
    "                     try:\n",
    "                         data['type'] = pred_data_df['type'][i]\n",
    "                     except:\n",
    "                         data['type'] = ''\n",
    "                         pass\n",
    "                     try:\n",
    "                         data['rank'] = rank\n",
    "                     except:\n",
    "                         data['rank'] = ''\n",
    "                         pass\n",
    "                     try:\n",
    "                         data['rr'] = rr\n",
    "                     except:\n",
    "                         data['rr'] = ''\n",
    "                         pass\n",
    "                     json_collection.append(data)\n",
    "            except Exception as ex:\n",
    "                   print(ex)\n",
    "                   pass\n",
    "            #summing reciprocal rank for calculation of Mean Reciprocal Rank(mrr)\n",
    "            sum_rr += rr\n",
    "        #print(str(self.cntr_dummy_ques)+' total predicted question types exist in original input file')\n",
    "        #print(str(cntr_ques_matched)+' correct question matches exist')\n",
    "        #print('No correct question matches exist for '+str(self.cntr_dummy_ques - cntr_ques_matched)+' questions')\n",
    "        catg_accuracy = round((catg_matched/self.cntr_dummy_ques),2)\n",
    "        mrr = round((sum_rr/cntr_ques_matched),2)\n",
    "        print('catg_matched = '+str(catg_accuracy))\n",
    "        print('mrr = '+str(mrr))\n",
    "\n",
    "        #closing input data files\n",
    "        data_file.close()\n",
    "        df_correct.close()\n",
    "\n",
    "        #writing out the json file appended with rank and rr\n",
    "        jf = open(out_file_path,'w')\n",
    "        json.dump(json_collection,jf,indent=6)\n",
    "        jf.close()\n",
    " \n",
    "        #writing out the results file having category accuracy and mrr\n",
    "        json_res_collection = []\n",
    "        dat = {}\n",
    "        dat['result_timestamp'] = str(datetime.datetime.now())\n",
    "        dat['categories_matched'] = catg_matched\n",
    "        dat['total_categories_present'] = self.cntr_dummy_ques\n",
    "        dat['category_matched_accuracy'] = catg_accuracy\n",
    "        dat['sum_rr'] = sum_rr\n",
    "        dat['type_matched'] = cntr_ques_matched\n",
    "        dat['type_mrr'] = mrr\n",
    "        json_res_collection.append(dat)\n",
    "               \n",
    "        #results_path = results_file + str(datetime.datetime.now()) + '.json'\n",
    "        results_path = results_file + '.json'   #removed date time on 19 Sept 2021 to reuse file to append results\n",
    "        \n",
    "        #added on 19th Sept to append rows of results\n",
    "        import os.path\n",
    "        if os.path.isfile(results_path):   #if the file exists\n",
    "           res_df = pd.read_json(results_path)\n",
    "           if res_df.empty:\n",
    "              jf = open(results_path,'w')\n",
    "              json.dump(json_res_collection,jf,indent=7)\n",
    "              jf.close()\n",
    "           else:\n",
    "              res_df = res_df.append(dat, ignore_index = True)\n",
    "              #df_temp = pd.DataFrame.from_dict(dat, orient = 'index')\n",
    "              #res_df = pd.concat([res_df, df_temp], ignore_index = True, axis = 0)\n",
    "              res_df.to_json(results_path,orient='records',indent=7)\n",
    "        else:\n",
    "           jf = open(results_path,'w')\n",
    "           json.dump(json_res_collection,jf,indent=2)\n",
    "           jf.close()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User file and evaluate it by instantiating above class functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>resource</td>\n",
       "      <td>[dbo:Organisation, dbo:Agent, dbo:Agent, dbo:A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>literal</td>\n",
       "      <td>[string]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  category                                               type\n",
       "0   4  resource  [dbo:Organisation, dbo:Agent, dbo:Agent, dbo:A...\n",
       "1   8   literal                                           [string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('../data_files/chaeyoon_system_output_json.json')\n",
    "print(len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catg_matched = 0.89\n",
      "mrr = 0.45\n"
     ]
    }
   ],
   "source": [
    "obj = fc_eval()\n",
    "file_name = 'XYZs_output'                                              #name of input file\n",
    "inp_file_path1 = '../data_files/task1_test_gold.json'          #gold file\n",
    "inp_file_path2 = '../data_files/'+file_name+'.json'\n",
    "out_file_path = '../data_files/'+file_name+'_with_rr_'+str(datetime.datetime.now())+'.json'\n",
    "results_file = '../data_files/'+file_name+'_final_scores'\n",
    "obj.evaluate(inp_file_path1,inp_file_path2,out_file_path,results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
